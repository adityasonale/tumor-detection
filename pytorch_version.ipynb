{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_vgg16_input(image):\n",
    "    # Convert image to numpy array and transpose dimensions\n",
    "    image = np.array(image)\n",
    "    image = image.transpose((2, 0, 1))  # Convert from HWC to CHW format\n",
    "\n",
    "    # Convert image to torch tensor\n",
    "    image = torch.tensor(image).float()\n",
    "\n",
    "    # Preprocess using mean subtraction\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).reshape(3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).reshape(3, 1, 1)\n",
    "    image = (image / 255.0 - mean) / std\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to 224x224\n",
    "    transforms.Lambda(lambda x: preprocess_vgg16_input(x)),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    # transforms.ToTensor()   # Convert images to tensors\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder already exists !!!\n",
      "Folder already exists !!!\n",
      "Folder already exists !!!\n",
      "Folder already exists !!!\n",
      "Folder already exists !!!\n",
      "Folder already exists !!!\n",
      "Folder already exists !!!\n",
      "Folder already exists !!!\n",
      "Folder already exists !!!\n"
     ]
    }
   ],
   "source": [
    "# training and testing directories\n",
    "\n",
    "def create_folder(x):\n",
    "    if os.path.exists(x):\n",
    "        print(\"Folder already exists !!!\")\n",
    "\n",
    "    else:\n",
    "        os.mkdir(x)\n",
    "        print(\"{x} created Successfully !!!\")\n",
    "\n",
    "\n",
    "folderpath_Train = r\"D:\\vs code\\python\\DeepLearning\\Projects\\Tumor\\train\"\n",
    "folderpath_Test = r\"D:\\vs code\\python\\DeepLearning\\Projects\\Tumor\\test\"\n",
    "folderpath_Validate = r\"D:\\vs code\\python\\DeepLearning\\Projects\\Tumor\\validate\"\n",
    "\n",
    "create_folder(folderpath_Train)\n",
    "create_folder(folderpath_Test)\n",
    "create_folder(folderpath_Validate)\n",
    "\n",
    "\n",
    "folder_path_train_damaged = r\"D:\\vs code\\python\\DeepLearning\\Projects\\Tumor\\train\\damaged\"\n",
    "folder_path_train_good = r\"D:\\vs code\\python\\DeepLearning\\Projects\\Tumor\\train\\good\"\n",
    "folder_path_test_damaged = r\"D:\\vs code\\python\\DeepLearning\\Projects\\Tumor\\test\\damaged\"\n",
    "folder_path_test_good = r\"D:\\vs code\\python\\DeepLearning\\Projects\\Tumor\\test\\good\"\n",
    "folder_path_validate_damaged = r\"D:\\vs code\\python\\DeepLearning\\Projects\\Tumor\\validate\\damaged\"\n",
    "folder_path_validate_good = r\"D:\\vs code\\python\\DeepLearning\\Projects\\Tumor\\validate\\good\"\n",
    "\n",
    "\n",
    "create_folder(folder_path_train_damaged)\n",
    "create_folder(folder_path_train_good)\n",
    "\n",
    "create_folder(folder_path_test_damaged)\n",
    "create_folder(folder_path_test_good)\n",
    "\n",
    "create_folder(folder_path_validate_good)\n",
    "create_folder(folder_path_validate_damaged)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.ImageFolder(root=folderpath_Train,transform=transform)\n",
    "\n",
    "test_dataset = datasets.ImageFolder(root=folderpath_Test,transform=transform)\n",
    "\n",
    "validation_dataset = datasets.ImageFolder(root=folderpath_Validate,transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset,batch_size=32, shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size= 32, shuffle= False)\n",
    "\n",
    "validate_loader = DataLoader(validation_dataset, batch_size=32, shuffle= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(64 * 28 * 28, 128)\n",
    "        self.fc2 = nn.Linear(128, 2)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "model = SimpleCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleCNN(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=50176, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing optimizer and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Training Loss: 0.6445\n",
      "Validation Loss: 0.5673, Accuracy: 0.7593\n",
      "Epoch 2/10, Training Loss: 0.3276\n",
      "Validation Loss: 0.2726, Accuracy: 0.8956\n",
      "Epoch 3/10, Training Loss: 0.2370\n",
      "Validation Loss: 0.2127, Accuracy: 0.9133\n",
      "Epoch 4/10, Training Loss: 0.1692\n",
      "Validation Loss: 0.1666, Accuracy: 0.9345\n",
      "Epoch 5/10, Training Loss: 0.1443\n",
      "Validation Loss: 0.1520, Accuracy: 0.9310\n",
      "Epoch 6/10, Training Loss: 0.0769\n",
      "Validation Loss: 0.1366, Accuracy: 0.9434\n",
      "Epoch 7/10, Training Loss: 0.0622\n",
      "Validation Loss: 0.1567, Accuracy: 0.9575\n",
      "Epoch 8/10, Training Loss: 0.0572\n",
      "Validation Loss: 0.1137, Accuracy: 0.9681\n",
      "Epoch 9/10, Training Loss: 0.0279\n",
      "Validation Loss: 0.1251, Accuracy: 0.9593\n",
      "Epoch 10/10, Training Loss: 0.0115\n",
      "Validation Loss: 0.1266, Accuracy: 0.9681\n",
      "Test Accuracy: 0.9699\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad() # Zero the parameter gradients\n",
    "        output = model(inputs)\n",
    "        loss = criterion(output,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step() # # Optimize\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    epoch_loss = running_loss/ len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    val_running_loss = 0.0\n",
    "    val_corrects = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in validate_loader:\n",
    "            outputs = model(inputs)  # Forward pass\n",
    "            loss = criterion(outputs, labels)  # Compute the loss\n",
    "            val_running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    # Calculate average validation loss and accuracy for the epoch\n",
    "    val_epoch_loss = val_running_loss / len(validate_loader.dataset)\n",
    "    val_accuracy = val_corrects.double() / len(validate_loader.dataset)\n",
    "    print(f\"Validation Loss: {val_epoch_loss:.4f}, Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "# Testing loop\n",
    "model.eval()  # Set model to evaluation mode\n",
    "test_corrects = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)  # Forward pass\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        test_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "# Calculate accuracy on test set\n",
    "test_accuracy = test_corrects.double() / len(test_loader.dataset)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
